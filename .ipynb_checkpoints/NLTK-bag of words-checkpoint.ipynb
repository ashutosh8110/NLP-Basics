{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=brown.words(categories=\"mystery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'were',\n",
       " 'thirty-eight',\n",
       " 'patients',\n",
       " 'on',\n",
       " 'the',\n",
       " 'bus',\n",
       " 'the',\n",
       " 'morning',\n",
       " 'I',\n",
       " 'left',\n",
       " 'for',\n",
       " 'Hanover',\n",
       " ',',\n",
       " 'most',\n",
       " 'of',\n",
       " 'them',\n",
       " 'disturbed',\n",
       " 'and',\n",
       " 'hallucinating',\n",
       " '.',\n",
       " 'An',\n",
       " 'interne',\n",
       " ',',\n",
       " 'a',\n",
       " 'nurse',\n",
       " 'and',\n",
       " 'two',\n",
       " 'attendants',\n",
       " 'were']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(words))\n",
    "words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Fulton',\n",
       "  'County',\n",
       "  'Grand',\n",
       "  'Jury',\n",
       "  'said',\n",
       "  'Friday',\n",
       "  'an',\n",
       "  'investigation',\n",
       "  'of',\n",
       "  \"Atlanta's\",\n",
       "  'recent',\n",
       "  'primary',\n",
       "  'election',\n",
       "  'produced',\n",
       "  '``',\n",
       "  'no',\n",
       "  'evidence',\n",
       "  \"''\",\n",
       "  'that',\n",
       "  'any',\n",
       "  'irregularities',\n",
       "  'took',\n",
       "  'place',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'jury',\n",
       "  'further',\n",
       "  'said',\n",
       "  'in',\n",
       "  'term-end',\n",
       "  'presentments',\n",
       "  'that',\n",
       "  'the',\n",
       "  'City',\n",
       "  'Executive',\n",
       "  'Committee',\n",
       "  ',',\n",
       "  'which',\n",
       "  'had',\n",
       "  'over-all',\n",
       "  'charge',\n",
       "  'of',\n",
       "  'the',\n",
       "  'election',\n",
       "  ',',\n",
       "  '``',\n",
       "  'deserves',\n",
       "  'the',\n",
       "  'praise',\n",
       "  'and',\n",
       "  'thanks',\n",
       "  'of',\n",
       "  'the',\n",
       "  'City',\n",
       "  'of',\n",
       "  'Atlanta',\n",
       "  \"''\",\n",
       "  'for',\n",
       "  'the',\n",
       "  'manner',\n",
       "  'in',\n",
       "  'which',\n",
       "  'the',\n",
       "  'election',\n",
       "  'was',\n",
       "  'conducted',\n",
       "  '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=brown.sents(categories=\"news\")\n",
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLP Pipeline\n",
    "* Data Collection\n",
    "* Tokenization,stopword removal,stemming\n",
    "* Building a common vocabulary\n",
    "* vectorize the documents\n",
    "* Perform Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"Dog is running on the grass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dog', 'is', 'running', 'on', 'the', 'grass']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen=\"Dog is running on the grass. Weather is awesome. Cat is bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dog is running on the grass.', 'Weather is awesome.', 'Cat is bad']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=\"I am not your yaar mind your langvez\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'yaar', 'mind', 'langvez']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in word_tokenize(sents) if i not in sw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming/Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "ss=SnowballStemmer(\"english\")\n",
    "Wnl=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"jumps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rose'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem(\"roses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simplest'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wnl.lemmatize(\"simplest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(s):\n",
    "    return [ps.stem(i) for i in s if  i not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'run', 'grass']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_words([\"dog\",\"is\",\"running\",\"on\",\"grass\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg=RegexpTokenizer(\"[a-zA-Z0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dog', 'is', 'running', '1', '2', '3', 'cat', 'is', 'also', 'running']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg.tokenize(\"Dog is running ,1,2,3, $^#@(*( cat is also running))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Common Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenize(s):\n",
    "    sent=rg.tokenize(s)\n",
    "    return filter_words(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\"Indian cricket Indian team will not win World Cup\",\n",
    "        \"We will win next Lok Sabha Election ,says Indian PM\",\n",
    "        \"Razzi is an exciting Indian spy movie based on real incident\",\n",
    "        \"Apj won heart of many Indians\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 16,\n",
       " 'cricket': 6,\n",
       " 'team': 49,\n",
       " 'will': 53,\n",
       " 'not': 32,\n",
       " 'win': 56,\n",
       " 'world': 61,\n",
       " 'cup': 8,\n",
       " 'indian cricket': 17,\n",
       " 'cricket indian': 7,\n",
       " 'indian team': 20,\n",
       " 'team will': 50,\n",
       " 'will not': 54,\n",
       " 'not win': 33,\n",
       " 'win world': 58,\n",
       " 'world cup': 62,\n",
       " 'we': 51,\n",
       " 'next': 30,\n",
       " 'lok': 24,\n",
       " 'sabha': 43,\n",
       " 'election': 9,\n",
       " 'says': 45,\n",
       " 'pm': 38,\n",
       " 'we will': 52,\n",
       " 'will win': 55,\n",
       " 'win next': 57,\n",
       " 'next lok': 31,\n",
       " 'lok sabha': 25,\n",
       " 'sabha election': 44,\n",
       " 'election says': 10,\n",
       " 'says indian': 46,\n",
       " 'indian pm': 18,\n",
       " 'razzi': 39,\n",
       " 'is': 22,\n",
       " 'an': 0,\n",
       " 'exciting': 11,\n",
       " 'spy': 47,\n",
       " 'movie': 28,\n",
       " 'based': 4,\n",
       " 'on': 36,\n",
       " 'real': 41,\n",
       " 'incident': 15,\n",
       " 'razzi is': 40,\n",
       " 'is an': 23,\n",
       " 'an exciting': 1,\n",
       " 'exciting indian': 12,\n",
       " 'indian spy': 19,\n",
       " 'spy movie': 48,\n",
       " 'movie based': 29,\n",
       " 'based on': 5,\n",
       " 'on real': 37,\n",
       " 'real incident': 42,\n",
       " 'apj': 2,\n",
       " 'won': 59,\n",
       " 'heart': 13,\n",
       " 'of': 34,\n",
       " 'many': 26,\n",
       " 'indians': 21,\n",
       " 'apj won': 3,\n",
       " 'won heart': 60,\n",
       " 'heart of': 14,\n",
       " 'of many': 35,\n",
       " 'many indians': 27}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc=cv.transform([\"I am an Indian\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['an', 'indian'], dtype='<U15')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(vc.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with our own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2=CountVectorizer(tokenizer=my_tokenize,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc=cv2.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14)\t2\n",
      "  (0, 4)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 42)\t1\n",
      "  (0, 44)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv2.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(tokenizer=my_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.41845521, 0.41845521, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.43673458, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.41845521,\n",
       "        0.3299149 , 0.41845521],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.38086157,\n",
       "        0.        , 0.        , 0.        , 0.19874937, 0.38086157,\n",
       "        0.        , 0.        , 0.38086157, 0.38086157, 0.        ,\n",
       "        0.        , 0.38086157, 0.38086157, 0.        , 0.        ,\n",
       "        0.30027564, 0.        ],\n",
       "       [0.        , 0.37082034, 0.        , 0.        , 0.        ,\n",
       "        0.37082034, 0.        , 0.37082034, 0.19350944, 0.        ,\n",
       "        0.        , 0.37082034, 0.        , 0.        , 0.37082034,\n",
       "        0.37082034, 0.        , 0.        , 0.37082034, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.55280532, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.55280532, 0.        , 0.28847675, 0.        ,\n",
       "        0.55280532, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
